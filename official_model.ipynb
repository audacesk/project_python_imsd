{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape I: Import des packages nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from tqdm import tqdm\n",
    "import urllib\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape II: Créer une fonction permettant d'automatiser le scrap par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_combinator_scraper():\n",
    "    \n",
    "    # Listing des URLs à scraper\n",
    "    based_url = 'https://www.ycdb.co/top-companies/'\n",
    "    url_funding = 'funding'\n",
    "    url_visited = 'alexa-rank'\n",
    "    url_popular = 'twitter-followers'\n",
    "    url_authoritative = 'domain-authority'\n",
    "    url_liked = 'facebook-likes'\n",
    "    url_upvoted = 'product-hunt-votes'\n",
    "    url_growing = 'growth-score'\n",
    "    url_prolific = 'tweet-count'\n",
    "    url_influential = 'linkedin-followers'\n",
    "    url_populous = 'employees'\n",
    "    url_exit_value = 'exit-value'\n",
    "\n",
    "    # Liste python regroupant les URLs mentionnées. Cette liste va permettre d'itérer la fonction sur chaque lien recherché\n",
    "    urls = [url_funding, url_visited, url_popular, url_authoritative, url_liked, url_upvoted, url_growing, url_prolific, url_influential, url_populous, url_exit_value]\n",
    "    \n",
    "    # A la fin de la fonction, cette variable sera incrémenter de +1 pour ajouter au nom du fichier excel exporté son numéro\n",
    "    rank = 0\n",
    "    \n",
    "    # Début de la bouble itérative\n",
    "    for i in urls:\n",
    "        # Appeler l'URL à scraper\n",
    "        url = based_url + i\n",
    "        response = get(url)\n",
    "\n",
    "        # Lancer une session BeautifulSoup, localiser puis stocker l'adresse de localisaiton du contenu de la page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')    \n",
    "        my_table = soup.find('table','table')\n",
    "\n",
    "        # Initialisation des listes que nous rempliront par la suite\n",
    "        noms = []\n",
    "        batch = []\n",
    "        categories = []\n",
    "        descriptions = []\n",
    "        variable = []\n",
    "        #images = []\n",
    "\n",
    "        # Répertorier les emplacements des éléments recherchés\n",
    "        names = my_table.find_all('a')\n",
    "        batches = my_table.find_all('td', 'd-none d-md-table-cell')[0::2]\n",
    "        category = my_table.find_all('td', 'd-none d-md-table-cell')[1::2]\n",
    "        description = my_table.find_all('td', 'd-none d-md-table-cell ellipsis')\n",
    "        text_right = my_table.find_all('td', 'text-right')\n",
    "        #image = my_table.find_all('img')\n",
    "\n",
    "        '''\n",
    "        Créer des boucles permettantes de parcourir l'arborescence de la page html et d'en sortir les informations\n",
    "        (ci-dessus) pour chaque lignes du classement\n",
    "        '''  \n",
    "\n",
    "        # Noms\n",
    "        for y in names:       \n",
    "            nam = y.text.strip()\n",
    "            noms.append(nam)\n",
    "\n",
    "        # Batch\n",
    "        for y in batches:\n",
    "            bat = y.text\n",
    "            batch.append(bat)\n",
    "\n",
    "        # Catégories\n",
    "        for y in category:\n",
    "            cat = y.text\n",
    "            categories.append(cat)\n",
    "\n",
    "        # Descriptions\n",
    "        for y in description:\n",
    "            des = y.text\n",
    "            descriptions.append(des)\n",
    "\n",
    "        # Élément différentiateur\n",
    "        for y in text_right:\n",
    "            lev = y.text\n",
    "            variable.append(lev)\n",
    "\n",
    "        '''\n",
    "        # Logo\n",
    "        for y in image:\n",
    "            img = y.get('src')\n",
    "            download_img = get(img)\n",
    "            images.append(download_img)\n",
    "        '''\n",
    "        \n",
    "        # Incrémentation de la variable 'rank'\n",
    "        rank += 1\n",
    "        \n",
    "        # Regroupement des listes dans un np.array\n",
    "        datas = np.column_stack((noms,batch,categories,descriptions,variable))\n",
    "        \n",
    "        # Transformation du np.array en un pd.DataFrame\n",
    "        df = pd.DataFrame(datas, columns=['noms', 'batch', 'categories', 'descriptions', i])\n",
    "        \n",
    "        # Export vers un fichier excel de nom 'ranking' + numéro du classement + nom du classement\n",
    "        df.to_excel(excel_writer= 'ranking_' + str(rank) + '_' + i + '.xlsx')\n",
    "\n",
    "        '''ATTENTION: les images seront à remettre dans le rendu final'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape III: Appel de la fonction de permettant de scraper les 11 classements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_combinator_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
